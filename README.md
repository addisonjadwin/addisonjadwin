# Addison Jadwin

## Bio
Hello! I'm Addison, a coterm masters student and ML researcher at Stanford. Below are some projects, experiences, and information demonstrating my experience!

## Projects
- **Improving minBERT Multitask Performance through In-Domain Pretraining and NRL Learning** - [Link](https://www.researchgate.net/publication/369901299_Improving_minBERT_Performance_on_Multiple_Tasks_through_In-domain_Pretraining_Negatives_Ranking_Loss_Learning_and_Hyperparameter_Optimization)
  - We implemented minBERT, a miniaturized version of BERT with less parameters, and demonstrated practices for improving multitask performance (sentiment analysis, paraphrase detection, and semantic textual similarity) across three different datasets. We improved minBERT embeddings by implementing a recently published loss funciton, Multiple Negatives Ranking Loss Learning, as well as with a masked-language-model task on the datasets, leading to improvements of up to 32% above baseline. 
- **AI Knowledge Navigator** - [Link](https://drive.google.com/file/d/120SoBaiETh0RUfio6yt_QHUd-CKgPBZM/view)
  - This is an LLM-powered agent I worked on which allows a user to quickly access and store semantic information across multiple documents. Users can query GPT-4 for desired information, both high-level or low-level, in their documents rather than manually searching for it. We found 4x acceleration compared to human reading speed. My role involved prompt engineering with the LLM to synthesize information from the documents and  backend indexing/storage of this information.
- **Predicting Right Ventricular Ejection Fraction Using a Convolutional Neural Learning Model** - [Link](http://dx.doi.org/10.13140/RG.2.2.22407.68007)
  - We built an R(2+1)D CNN which, using transfer learning and 10-fold cross-validation, could predict right ventricular ejection fraction (RVEF), a measure of heart health, from echocardiogram videos. The dataset was mainly labeled with *left* ventricular ejection fraction (LVEF), a more common metric, so it involved using transfer learning from the optimal weights for LVEF in order to achieve high performance on the small RVEF dataset.
- **Helping and Hindering: Recursive Reasoning in a Multi-Agent MDP** - [Link](https://www.researchgate.net/publication/369901489_Helping_and_Hindering_Recursive_Reasoning_in_a_Multi-Agent_MDP)
  - We modeled 3rd-order recursive reasoning (reasoning about how another agent reasons about you) in a helping/hindering grid world scenario. Using online MDP planning algorithms, we were able to simulate intelligently-behaving agents which outperformed baseline models.
- **Emotion Classification on EEG BMI Data with an LSTM+Conv Architecture** - [Link](https://drive.google.com/file/d/1Xu2JApRhIVjK-7Tp3H4m9KAKj7qv11nb/view?usp=sharing)
  - We beat the SOTA on an EEG brainwave dataset on emotion classification using a novel architecture with a combination of convolutional and LSTM layers, achieving 99.2% accuracy on the test data.
- **Categorization of Objects Through Causal Properties in Recurrent Neural Networks** - [Link](https://drive.google.com/file/d/1YrOp5wnt-EOAnRDK9k_ky8hkbv3OwAoe/view?usp=sharing)
  - I construct small 'object worlds' and train an RNN to distinguish the different objects by their causal properties (i.e. which other objects they effect and which objects effect them) by observing sequences of events. I also analyze the representations the RNN learns of the objects and how it learns them with different mathematical techniques.
- **Predictive Capabilities of Neural Network Language Models on Brain Data** - [Link](https://drive.google.com/file/d/1r3z0lM2dEEdqnnj3czdZkAnxVQou9zzr/view?usp=sharing)
  - I review the ability of language models (LLMs as well as early models like simple RNNs or word vector models) in predicting neural language processing data.
- **Self-Supervised & Unsupervised Neural Network Models of Auditory Processing in the Brain** - [Link](https://drive.google.com/file/d/1IxNdWHx69o_WgO0u4hRTnySVnehMEsp9/view?usp=sharing)
  - I review the ability of self-supervised/unsupervised models of auditory language data such as HuBERT or wav2vec in predicting neural audio processing data. 



## Education
- **[Degree] in [Field of Study]** - [University/College Name] (Year of Graduation)
  - [Optional: Brief description of relevant coursework, honors, or achievements.]
- **[Degree] in [Field of Study]** - [University/College Name] (Year of Graduation)
  - [Optional: Brief description of relevant coursework, honors, or achievements.]

## Work Experience
- **[Job Title]** - [Company Name] (Month Year - Month Year)
  - [Brief description of your responsibilities and achievements in this role.]
- **[Job Title]** - [Company Name] (Month Year - Month Year)
  - [Brief description of your responsibilities and achievements in this role.]

## Skills
- Programming Languages: [List of programming languages you're proficient in]
- Tools & Technologies: [List of tools, frameworks, libraries, etc. you're experienced with]
- Other Skills: [Any other relevant skills]

## Contact
- LinkedIn: [Your LinkedIn Profile](URL)
- GitHub: [Your GitHub Profile](URL)
- Email: [Your Email Address](mailto:youremail@example.com)


<!--
**addisonjadwin/addisonjadwin** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
